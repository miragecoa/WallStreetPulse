Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>'''\r\nimport reddit_api\r\nimport gpt_api\r\n\r\n\r\n# Sample usages\r\nposts = reddit_api.get_hot_posts()\r\n# print(posts.json())\r\n\r\n\r\nresponse, chat_history = gpt_api.get_response(\"What is RCOS\")\r\n\r\nresponse, chat_history = gpt_api.get_response(\"What is WallStreetPulse\")\r\n'''\r\n\r\nfrom Reddit_Posts import Reddit_Posts\r\nfrom datetime import datetime, timedelta\r\n\r\ndef main():\r\n    posts = Reddit_Posts(num_posts=100, subreddit_name=\"wallstreetbets\")\r\n    '''\r\n    ###test to see posts titles, authors, and comments works\r\n    print(f\"Title: {posts.get_title(1)}\")\r\n    print(f\"Author: {posts.get_author(1)}\")\r\n    print(posts.get_content(2))\r\n    print(f\"First hot comment: {posts.get_comments(1, 1)[0]['content']}\")\r\n\r\n    ###test to see if a specific user's post frequency is correct\r\n    username_to_check = \"OPINION_IS_UNPOPULAR\"\r\n    frequency_for_user = posts.get_user_post_frequency(username_to_check, time_frame_days)\r\n    print(f\"The frequency of posts by {username_to_check} in the chosen subreddit in the last {time_frame_days} days is: {frequency_for_user}\")\r\n    posts_titles_within_time_frame = [post.title for post in posts.posts if\r\n                                      post.created_utc > timestamp_limit.timestamp() \r\n                                      and post.author and post.author.name == username_to_check]\r\n    print(f\"Titles of posts by {username_to_check} in the last {time_frame_days} days:\")\r\n    for title in posts_titles_within_time_frame:\r\n        print(f\"- {title}\")\r\n    \r\n    ###test to see every author's posts that we grab from the desired number of votes\r\n    time_frame_days = 3\r\n    authors_frequency = posts.get_all_authors_post_frequency(time_frame_days)\r\n    timestamp_limit = datetime.utcnow() - timedelta(days=time_frame_days)\r\n\r\n    # Print the results\r\n    for author, frequency in authors_frequency.items():\r\n        print(f\"The frequency of posts by {author} in the chosen subreddit in the last {time_frame_days} days is: {frequency}\")\r\n\r\n        # Print titles of posts by the specified author within the time frame\r\n        posts_titles_within_time_frame = [post.title for post in posts.posts if\r\n                                           post.created_utc > timestamp_limit.timestamp() and\r\n                                           post.author and\r\n                                           post.author.name == author]\r\n        print(f\"Titles of posts by {author} in the last {time_frame_days} days:\")\r\n        for title in posts_titles_within_time_frame:\r\n            print(f\"- {title}\")\r\n    \r\n    ###test to see if all post stats are correct\r\n    time_frame_days = 3\r\n    # Get the timestamp for the start of the time frame\r\n    timestamp_limit = datetime.utcnow() - timedelta(days=time_frame_days)\r\n\r\n    # Get post frequency, average upvotes per post, upvote to downvote ratio per post,\r\n    # and average comments per post for each unique author in the specified time frame\r\n    authors_frequency, authors_average_upvotes, authors_upvote_to_downvote_ratio, authors_average_comments = posts.get_all_authors_post_stats(time_frame_days)\r\n\r\n    # Print the results\r\n    for author, frequency in authors_frequency.items():\r\n        print(f\"The frequency of posts by {author} in the chosen subreddit in the last {time_frame_days} days is: {frequency}\")\r\n\r\n        # Print average upvotes per post for each author\r\n        average_upvotes = authors_average_upvotes.get(author, 0)\r\n        print(f\"The average upvotes per post for {author} in the last {time_frame_days} days is: {average_upvotes}\")\r\n\r\n        # Print upvote to downvote ratio per post for each author\r\n        upvote_to_downvote_ratio = authors_upvote_to_downvote_ratio.get(author, 0)\r\n        print(f\"The upvote to downvote ratio per post for {author} in the last {time_frame_days} days is: {upvote_to_downvote_ratio}\")\r\n\r\n        # Print average comments per post for each author\r\n        average_comments = authors_average_comments.get(author, 0)\r\n        print(f\"The average comments per post for {author} in the last {time_frame_days} days is: {average_comments}\")\r\n    '''\r\n    time_frame_days = 3\r\n    # Get post frequency, average upvotes per post, upvote to downvote ratio per post,\r\n    # and average comments per post for each unique author in the specified time frame\r\n    authors_frequency, authors_average_upvotes, authors_upvote_to_downvote_ratio, authors_average_comments = posts.get_all_authors_post_stats(time_frame_days)\r\n\r\n   \r\n    author_scores = posts.calculate_author_scores(authors_frequency, authors_average_upvotes, authors_upvote_to_downvote_ratio, authors_average_comments)\r\n\r\n    # Print the composite scores\r\n    for author, score in author_scores.items():\r\n        print(f\"The composite score for {author} is: {score}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.py b/main.py
--- a/main.py	(revision 1361bba6ea76b25e50847436d11eaac9163b2be8)
+++ b/main.py	(date 1708765569631)
@@ -1,30 +1,30 @@
-'''
 import reddit_api
+'''
 import gpt_api
-
+'''
 
 # Sample usages
-posts = reddit_api.get_hot_posts()
+# posts = reddit_api.get_hot_posts()
 # print(posts.json())
 
-
+'''
 response, chat_history = gpt_api.get_response("What is RCOS")
-
 response, chat_history = gpt_api.get_response("What is WallStreetPulse")
 '''
 
+
 from Reddit_Posts import Reddit_Posts
 from datetime import datetime, timedelta
 
 def main():
     posts = Reddit_Posts(num_posts=100, subreddit_name="wallstreetbets")
-    '''
+
     ###test to see posts titles, authors, and comments works
     print(f"Title: {posts.get_title(1)}")
     print(f"Author: {posts.get_author(1)}")
     print(posts.get_content(2))
     print(f"First hot comment: {posts.get_comments(1, 1)[0]['content']}")
-
+    
     ###test to see if a specific user's post frequency is correct
     username_to_check = "OPINION_IS_UNPOPULAR"
     frequency_for_user = posts.get_user_post_frequency(username_to_check, time_frame_days)
@@ -53,7 +53,7 @@
         print(f"Titles of posts by {author} in the last {time_frame_days} days:")
         for title in posts_titles_within_time_frame:
             print(f"- {title}")
-    
+
     ###test to see if all post stats are correct
     time_frame_days = 3
     # Get the timestamp for the start of the time frame
@@ -78,7 +78,7 @@
         # Print average comments per post for each author
         average_comments = authors_average_comments.get(author, 0)
         print(f"The average comments per post for {author} in the last {time_frame_days} days is: {average_comments}")
-    '''
+    
     time_frame_days = 3
     # Get post frequency, average upvotes per post, upvote to downvote ratio per post,
     # and average comments per post for each unique author in the specified time frame
@@ -92,4 +92,4 @@
         print(f"The composite score for {author} is: {score}")
 
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()
Index: redditData/google_search.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/redditData/google_search.py b/redditData/google_search.py
new file mode 100644
--- /dev/null	(date 1709069747343)
+++ b/redditData/google_search.py	(date 1709069747343)
@@ -0,0 +1,42 @@
+
+# https://developers.google.com/custom-search/v1/overview
+# search engine id: b3dc3b1ce374e440c
+# api key: AIzaSyBG2uCYJDwpZLlVcsmracUk3zRSJZMpn98
+import requests
+from datetime import datetime
+
+# Define the base URL for the Custom Search JSON API
+base_url = "https://www.googleapis.com/customsearch/v1"
+
+# params
+#   start_date: string with format %Y%m%d
+#   end_date: string with format %Y%m%d
+# returns
+# a list of all article id during the given time period
+def search_by_time_period(start_date = "20231203", end_date = "20240110"):
+    article_ids = []
+    num = 10
+    while(int(start_date) < int(end_date) and num == 10):
+        input()
+        print('req')
+        params = {
+            "key": "AIzaSyBG2uCYJDwpZLlVcsmracUk3zRSJZMpn98", # The API key
+            "cx": "b3dc3b1ce374e440c", # The CSE ID
+            "q": "reddit", # The search query
+            "sort": f"date:r:{start_date}:{end_date}", # The date range filter
+            "num": 10 # The number of results to return
+        }
+        response = requests.get(base_url, params=params)
+        if response.status_code == 200:
+            data = response.json()
+            num = len(data["items"])
+            for item in data["items"]:
+                article_ids.append(item['link'].split('/')[-3])
+                date_obj = datetime.strptime(item['snippet'][:12].lstrip().rstrip(), "%b %d, %Y")
+                start_date = max(date_obj.strftime("%Y%m%d"),start_date)
+        else:
+            print(f"Request failed with status code {response.status_code}")
+    return article_ids
+
+if __name__ == "__main__":
+    search_by_time_period()
\ No newline at end of file
Index: redditData/article_id_decoder.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/redditData/article_id_decoder.py b/redditData/article_id_decoder.py
new file mode 100644
--- /dev/null	(date 1708739097214)
+++ b/redditData/article_id_decoder.py	(date 1708739097214)
@@ -0,0 +1,35 @@
+def base36encode(integer: int) -> str:
+    chars = '0123456789abcdefghijklmnopqrstuvwxyz'
+    sign = '-' if integer < 0 else ''
+    integer = abs(integer)
+    result = ''
+    while integer > 0:
+        integer, remainder = divmod(integer, 36)
+        result = chars[remainder] + result
+    return sign + result
+
+
+def base36decode(base36: str) -> int:
+    return int(base36, 36)
+
+
+def get_next_ids(start_id, count):
+    start_num = base36decode(start_id)
+    ids = []
+    id_num = -1
+    for id_num in range(start_num, start_num + count):
+        ids.append(base36encode(id_num))
+    return ids, base36encode(id_num)
+
+
+def get_prev_ids(start_id, count):
+    start_num = base36decode(start_id)
+    ids = []
+    id_num = -1
+    for id_num in range(start_num - 1, start_num - count - 1, -1):
+        ids.append(base36encode(id_num))
+    return ids, base36encode(id_num)
+
+
+
+
Index: redditData/note.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/redditData/note.txt b/redditData/note.txt
new file mode 100644
--- /dev/null	(date 1708731559811)
+++ b/redditData/note.txt	(date 1708731559811)
@@ -0,0 +1,2 @@
+Please put every reddit api/data process codes here and make appropriate changes
+Just for better organize
\ No newline at end of file
Index: gpt_api.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\r\nimport openai\r\nfrom langchain.chains import ConversationalRetrievalChain, RetrievalQA\r\nfrom langchain.chat_models import ChatOpenAI\r\nfrom langchain.document_loaders import DirectoryLoader, TextLoader\r\nfrom langchain.embeddings import OpenAIEmbeddings\r\nfrom langchain.indexes import VectorstoreIndexCreator\r\nfrom langchain.indexes.vectorstore import VectorStoreIndexWrapper\r\nfrom langchain.vectorstores import Chroma\r\nimport time\r\n\r\n\r\n# https://platform.openai.com/api-keys\r\napi_base = \"https://api.openai.com/v1\"\r\nwith open('apikey.txt', 'r') as f:\r\n    api_key = f.read()\r\n\r\n# Types of model to be used, can be found at https://platform.openai.com/docs/models/overview\r\nmodel_type = \"gpt-3.5-turbo\"\r\n# A director containing an Index folder that stores the previous generated vectors\r\npersist_directory = './data'\r\n# Director containing all sources files to be used when sending the prompt\r\nsource_directory = './data/source'\r\n# Top k numbers of documents to be dynamically loaded\r\ntarget_source_chunks = 1\r\n\r\n\r\n\r\n# Enable to save to disk & reuse the model (for repeated queries on the same data)\r\nPERSIST = False\r\n\r\nos.environ[\"OPENAI_API_KEY\"] = api_key\r\nopenai.api_base = api_base\r\n\r\nif PERSIST and os.path.exists(\"persist\"):\r\n    print(\"Reusing index...\\n\")\r\n    vectorstore = Chroma(persist_directory=\"persist\", embedding_function=OpenAIEmbeddings())\r\n    index = VectorStoreIndexWrapper(vectorstore=vectorstore)\r\nelse:\r\n    loader = DirectoryLoader(source_directory)\r\n    if PERSIST:\r\n        index = VectorstoreIndexCreator(vectorstore_kwargs={\"persist_directory\": \"persist\"}).from_loaders([loader])\r\n    else:\r\n        index = VectorstoreIndexCreator().from_loaders([loader])\r\n\r\nchain = ConversationalRetrievalChain.from_llm(\r\n    llm=ChatOpenAI(model=model_type),\r\n    retriever=index.vectorstore.as_retriever(search_kwargs={\"k\": target_source_chunks}),\r\n)\r\n\r\n\r\n\r\n# Generate response using gpt api with input query and chat history\r\ndef get_response(query, chat_history=[]):\r\n    print(\"\\n> Input received\")\r\n\r\n    similar_docs = chain.retriever.get_relevant_documents(query)\r\n    # Print the similar documents found by vector retriever\r\n    print(\"Similar documents:\")\r\n    for doc in similar_docs:\r\n        print(doc)\r\n\r\n\r\n    start = time.time()\r\n    answer = chain({\"question\": query, \"chat_history\": chat_history})\r\n    end = time.time()\r\n\r\n    print(\"> Question:\")\r\n    print(query)\r\n    print(f\"> Answer (took {round(end - start, 2)} s.):\")\r\n    print(answer['answer'])\r\n\r\n    chat_history.append((query, answer['answer']))\r\n\r\n    return answer, chat_history\r\n\r\n\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/gpt_api.py b/gpt_api.py
--- a/gpt_api.py	(revision 1361bba6ea76b25e50847436d11eaac9163b2be8)
+++ b/gpt_api.py	(date 1708730333447)
@@ -75,4 +75,3 @@
     return answer, chat_history
 
 
-
Index: README.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># WallStreetPulse\r\nI'm using Python 3.11 for this project. Other versions should also work, but there might be some dependency issues.\r\n\r\npip install -r requirements.txt\r\n\r\nhttps://stackoverflow.com/a/76245995 could solve chromadb problem\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/README.md b/README.md
--- a/README.md	(revision 1361bba6ea76b25e50847436d11eaac9163b2be8)
+++ b/README.md	(date 1708723997995)
@@ -4,3 +4,4 @@
 pip install -r requirements.txt
 
 https://stackoverflow.com/a/76245995 could solve chromadb problem
+
diff --git a/redditData/test_codes.py b/redditData/test_codes.py
new file mode 100644
